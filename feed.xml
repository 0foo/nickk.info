<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2023-12-15T14:50:09+00:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Your awesome title</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">Scraping all of a users comments in Reddit and uploading to chatgpt</title><link href="http://0.0.0.0:4000/2023/11/20/reddit-gpt.html" rel="alternate" type="text/html" title="Scraping all of a users comments in Reddit and uploading to chatgpt" /><published>2023-11-20T12:00:00+00:00</published><updated>2023-11-20T12:00:00+00:00</updated><id>http://0.0.0.0:4000/2023/11/20/reddit-gpt</id><content type="html" xml:base="http://0.0.0.0:4000/2023/11/20/reddit-gpt.html"><![CDATA[<p>I wrote a reddit comment scraper and then uploaded the results of the user comment scraping to ChatGPT4 to see what I could learn about the user.</p>

<ul>
  <li>Code/Files in this experiment:
    <ul>
      <li><a href="https://github.com/0foo/reddit_comment_scraper/tree/master">https://github.com/0foo/reddit_comment_scraper/tree/master</a></li>
      <li><a href="https://github.com/0foo/reddit_comment_scraper/blob/master/data/spez.txt">https://github.com/0foo/reddit_comment_scraper/blob/master/data/spez.txt</a></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<p><u> Chatting with GPT </u></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(uploaded file spez.txt)
Me: Please analyze this file and give me a summary of the person.
</code></pre></div></div>

<!--more-->

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ChatGPT: The file "spez.txt" contains a series of Reddit comments attributed to the user 'spez'. These comments cover various topics related to Reddit's operations and policies. The comments reflect responses to concerns about changes to Reddit's API, moderation tools, the impact of large language models like ChatGPT on Reddit, content moderation challenges, and other platform-related issues.

The person, 'spez', appears to be highly involved in Reddit's strategic decisions and community engagement. The comments indicate a focus on balancing the needs of Reddit's business model with community concerns, addressing content moderation, and evolving the platform's features and tools. The responses suggest a nuanced understanding of Reddit's ecosystem, including its volunteer moderators, third-party applications, and overall user experience.
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Me: Can you tell me who spez might be
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Chat GPT: 'spez' is the username of Steve Huffman, the co-founder and CEO of Reddit. Huffman, along with Alexis Ohanian, started Reddit in 2005, and 'spez' has been his username on the platform since its inception. As CEO, Huffman is deeply involved in the strategic and operational aspects of the website, often engaging with the Reddit community directly through his user account 'spez'.
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Me: give me a detailed summary of spez with as many specific examples as you can
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Chat GPT: Certainly, here are specific examples from the "spez.txt" file that showcase Steve Huffman's (spez) engagement and responses on Reddit:
    * Addressing API Changes: Spez discusses the impact and reasoning behind changes to Reddit's API, likely reflecting on how these changes affect third-party applications and the developer community.
    * Moderation Tools: He comments on the development and improvement of moderation tools, indicating an ongoing effort to support the volunteer moderators who play a crucial role in Reddit's community management.
    * Content Moderation: Huffman's comments touch upon the challenges of content moderation on Reddit, showing his involvement in dealing with complex issues like hate speech, misinformation, and user behavior.
    * Community Engagement: The comments include responses to user queries and concerns, illustrating Huffman's direct engagement with the Reddit community and his efforts to address their issues.

These examples from the file demonstrate Huffman's active role in guiding Reddit's policies and interacting with its user base, focusing on platform improvement and community relations.
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Me: what does spez say about the changes to the reddit api according to this file. be specific with examples
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Chat GPT: In the file "spez.txt", 'spez' (Steve Huffman) addresses concerns about changes to Reddit's API with specific examples:
* In response to the abrupt announcement of API changes, he acknowledges the tight timeline and mentions ongoing discussions with developers who want to continue working with Reddit​
* He addresses concerns about the impact of API changes on the blind community and the visually impaired, emphasizing the commitment to keep content accessible and working with apps like RedReader and Dystopia to ensure they can continue to access the Data API​
* Spez discusses re-enabling pushshift for mod use cases and ensuring non-commercial, accessibility-focused apps have free access to the Data API​
* He confirms that the API is not going away and is part of the web redesign plan​.
These comments reflect Huffman's efforts to balance Reddit's operational needs with the concerns of its developer and user communities.
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Me: Can you give me a list of comments where spez curses
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Chat GPT: Here is a summary of comments from the file "spez.txt" where the user 'spez' uses cursing language:

"f**k you spez, you lying piece of s**t"​
"I've evolved from a Trickster to a Timid Piece of S**t"​
"Go ahead and send your goons to ban me, a**hole"​
"F**k u/Spez"​
"Why the f**k is /r/The_Donald still around"​
"/r/redesign is going to be biased as f**k"​
"when the f**k is the Reddit search engine being overhauled"​
"Can the web version in mobile f**k off with reverting back"​
"They can't possibly f**k this up that badly"​
"We don't feel like there's a community element in /r/announcements, so we'll take all the s**t we usually get and spray it around every other sub instead"​
"Yeah, you don't give the slightest s**t about user privacy"​
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Me: the comment_parent field is not spez
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Chat GPT: I apologize for the oversight. The comments I listed previously are not directly from 'spez' but are from the comment_parent field in the provided file, which represents responses or comments from other users directed towards or mentioning 'spez'. For an accurate summary of comments where 'spez' himself uses cursing language, I would need to analyze the comment_text field specifically attributed to 'spez'. Let me re-examine the file with this correct focus.
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Me: please re-examine
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Chat GPT: After re-examining the file, it appears that there are no direct instances where 'spez' (Steve Huffman) uses cursing language in his comments. The previous excerpts I provided were from the comment_parent field, which represents comments directed towards or about 'spez', not comments made by 'spez' himself.

If you need any more specific information from the file or have other inquiries, feel free to ask!
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Me: can you list all of the fields in this file
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Chat GPT: The fields in the "spez.txt" file, based on the structure of the comments, are as follows:

    comment_title: The title of the Reddit post or thread where the comment was made.
    comment_text: The actual text of the comment made by 'spez'.
    comment_id: The unique identifier for the comment on Reddit.
    comment_context_link: The URL to the specific comment on Reddit, providing context for where the comment appears.
    comment_parent: The text of the parent comment or post to which 'spez' is responding, if applicable.

These fields collectively provide a structured view of each comment, its context, and its location within the Reddit platform.
</code></pre></div></div>

<p>The most interesting aspect in my opinion is the ability to recognize fields once prompted with zero assistance and learn about them for future actions.</p>

<p>I noticed it was limited to the examples it could provide, I assuming that’s because the text is so large and it has a limited context. 
This weakens it for large scale information analysis in my opinion….at the moment.
Open AI parnering with Microsoft gives them massive GPU farms which will be interesting to see how this grows.</p>]]></content><author><name></name></author><category term="software," /><category term="scraping," /><category term="gpt" /><summary type="html"><![CDATA[I wrote a reddit comment scraper and then uploaded the results of the user comment scraping to ChatGPT4 to see what I could learn about the user. Code/Files in this experiment: https://github.com/0foo/reddit_comment_scraper/tree/master https://github.com/0foo/reddit_comment_scraper/blob/master/data/spez.txt Chatting with GPT (uploaded file spez.txt) Me: Please analyze this file and give me a summary of the person.]]></summary></entry><entry><title type="html">Working around API request limits with bulk IP proxy</title><link href="http://0.0.0.0:4000/2023/11/16/api-limits.html" rel="alternate" type="text/html" title="Working around API request limits with bulk IP proxy" /><published>2023-11-16T12:06:00+00:00</published><updated>2023-11-16T12:06:00+00:00</updated><id>http://0.0.0.0:4000/2023/11/16/api-limits</id><content type="html" xml:base="http://0.0.0.0:4000/2023/11/16/api-limits.html"><![CDATA[<p><em>( Note: this post is purely for experimentation and learning purposes. Sites have rate limits specifically to avoid data collection, because their data is propietary and core to their business. Additionally, on smaller sites especially, excessive scraping can DDOS the site and cause expensive resource utilization. Please respect a web site’s wishes. )</em>
<br /><br /></p>

<p>Most websites have API request limits <strong>based on IP</strong> which make scraping or bulk data collection on their sites prohibitive.</p>

<p>Scraping and data collection requires a LOT of requests for each new page of data which would be rather quickly rate limited, slowing down your data collection to unsatisfactory speeds.</p>

<p>So what’s the solution?  Send each request through a unique IP!</p>

<p>There exists companies which specialize in Bulk IP proxying, and have bulk amounts of IP’s, which, for a small fee, will let you proxy all of your requests through their network which, if all goes well, should give you unique IP per request goodness.</p>

<p>I found one of these IP providers called IPRoyal.</p>

<p>IPRoyal claims to have a network of 8 million residential IP’s!!!</p>

<p>I signed up for IPRoyal and forked over the 7 bucks to see what happens.</p>

<!--more-->

<hr class="article-separator-more" />

<p>To begin, let’s run a non-proxied reddit.com query with a curl statment to see how quickly we get rate limited.</p>

<p>As everyone knows, 200 status code is a successful request and 429 status code is a blocked reqest due to rate limit.</p>

<p>Let’s see if we will get blocked by reddit.</p>

<p>(Note: These are async responses, so the responses are unordered. Had to async these for speed. )</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nick@nick-XPS-9315:~<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..1000<span class="o">}</span><span class="p">;</span> <span class="k">do</span> /bin/bash <span class="nt">-c</span> <span class="s1">'curl  -sL -o /dev/null -w "%{http_code} " https://www.reddit.com &amp;'</span><span class="p">;</span> <span class="k">done</span><span class="p">;</span>
nick@nick-XPS-9315:~<span class="nv">$ </span>429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 200 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 429 200 429 429 429 429 429 429 429 429 429 429 429 429 429 429 200 429 429 200 429 200 200 429 200 200 200 429 200 429 429 200 429 200 200 200 429 429 200 429 200 200 200 429....
</code></pre></div></div>

<p>Rate limited almost immediately lol.</p>

<hr class="article-separator-more" />

<p>So let’s try with the IPRoyal network.</p>

<p>So, for 7 bucks IPRoyal gives you 1gig worth of data.</p>

<p>To use their network you are provided with a hash/url combo which you can configure as a proxy via proxy application or combine for curl.</p>

<p>Now let’s try with the IPRoyal proxy. :) (-x being the curl proxy flag)</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">IPRoyalProxy</span><span class="o">=</span><span class="s2">"http://&lt;someBigHash&gt;@geo.iproyal.com:12321"</span>
nick@nick-XPS-9315:~<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..1000<span class="o">}</span><span class="p">;</span> <span class="k">do</span> /bin/bash <span class="nt">-c</span> <span class="s2">"curl  -s -o /dev/null -w </span><span class="se">\"</span><span class="s2">%{http_code} </span><span class="se">\"</span><span class="s2"> -I -x </span><span class="nv">$IPRoyalProxy</span><span class="s2"> -L https://www.reddit.com &amp;"</span><span class="p">;</span> <span class="k">done</span><span class="p">;</span>
nick@nick-XPS-9315:~<span class="nv">$ </span>200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 000 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 .....
</code></pre></div></div>

<p><br />
Not a single 429!! <br />
IPRoyal is working as advertised.<br />
In the next post we’re going to combine this with some sort of scraper or web spider to try and scrape all of the comment’s of a Reddit user.</p>]]></content><author><name></name></author><category term="networking" /><summary type="html"><![CDATA[( Note: this post is purely for experimentation and learning purposes. Sites have rate limits specifically to avoid data collection, because their data is propietary and core to their business. Additionally, on smaller sites especially, excessive scraping can DDOS the site and cause expensive resource utilization. Please respect a web site’s wishes. ) Most websites have API request limits based on IP which make scraping or bulk data collection on their sites prohibitive. Scraping and data collection requires a LOT of requests for each new page of data which would be rather quickly rate limited, slowing down your data collection to unsatisfactory speeds. So what’s the solution? Send each request through a unique IP! There exists companies which specialize in Bulk IP proxying, and have bulk amounts of IP’s, which, for a small fee, will let you proxy all of your requests through their network which, if all goes well, should give you unique IP per request goodness. I found one of these IP providers called IPRoyal. IPRoyal claims to have a network of 8 million residential IP’s!!! I signed up for IPRoyal and forked over the 7 bucks to see what happens.]]></summary></entry><entry><title type="html">Weather Data Mining</title><link href="http://0.0.0.0:4000/2023/09/06/weather-data-mining.html" rel="alternate" type="text/html" title="Weather Data Mining" /><published>2023-09-06T12:16:00+00:00</published><updated>2023-09-06T12:16:00+00:00</updated><id>http://0.0.0.0:4000/2023/09/06/weather-data-mining</id><content type="html" xml:base="http://0.0.0.0:4000/2023/09/06/weather-data-mining.html"><![CDATA[<p>In order to find places to live or visit where I can maximize the amount of time I’m able to comfortably spend outside I wrote a script to calculate the locations in the US with temperatures between 30 degrees and 75 degrees for 6 months or more.
<br /><br /></p>

<ul>
  <li>All the data comes from NOAA weather stations and can be downloaded here: <a href="https://www.ncei.noaa.gov/pub/data/ghcn/daily/">link</a></li>
  <li>The github repo is here: <a href="https://github.com/0foo/weather_mining">https://github.com/0foo/weather_mining</a></li>
  <li>The interactive google map is here: <a href="https://www.google.com/maps/d/edit?mid=11qYJOQ2cX9j3T-x5UO5ykXmjUToSdEs&amp;ll=42.70028862623725%2C-119.24163318010127&amp;z=7">link</a>
<br /><br /></li>
</ul>

<p><!--more--></p>

<p><u>Interesting observations:</u></p>

<ul>
  <li>
    <p>Looks like the crown for perfect temps year round goes to the Pacific Northwest including northern California, Oregon, and Washington.  Many locations there have perfect temps 12 months out of the year.</p>
  </li>
  <li>
    <p>The entire West coast fared really well with most of it experiencing comfortable temps most of the year with a few months slightly deviating into cold or hot.</p>
  </li>
  <li>
    <p>There’s a swath of the southeastern part of the country that has more than 6 months of comfortable temps that curves up the east coast, however almost all of the east/southeast locations cap out at 7 or 8 months of comfortable temps, which doesn’t reach west coast comfort levels of 8+ months typically.</p>
  </li>
</ul>

<p><br /></p>

<p><u>To Do (when have time) </u></p>
<ul>
  <li>At some point would like to convert this to a dynamic site where you can plot migration patterns to stay in comfortable weather for traveling retired or digital nomads.</li>
  <li>Would like to be able to dynamically input your own custom comfortable temp ranges as peoples idea of comfortable temps and temp tolerances may vary.</li>
  <li>At some point would like to include other weather factors like sunshine, preciptation, snowfall</li>
</ul>

<p><br /><br /></p>

<ul>
  <li>Note: on this map the field labeled count is months within the comfort zone of 30 to 75.
<br /></li>
</ul>
<iframe src="https://www.google.com/maps/d/embed?mid=11qYJOQ2cX9j3T-x5UO5ykXmjUToSdEs&amp;ehbc=2E312F" width="100%" height="800"></iframe>]]></content><author><name></name></author><summary type="html"><![CDATA[In order to find places to live or visit where I can maximize the amount of time I’m able to comfortably spend outside I wrote a script to calculate the locations in the US with temperatures between 30 degrees and 75 degrees for 6 months or more. All the data comes from NOAA weather stations and can be downloaded here: link The github repo is here: https://github.com/0foo/weather_mining The interactive google map is here: link]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://0.0.0.0:4000/noaa.jpeg" /><media:content medium="image" url="http://0.0.0.0:4000/noaa.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Terrible Implementations of Is Even</title><link href="http://0.0.0.0:4000/2023/09/02/is-even.html" rel="alternate" type="text/html" title="Terrible Implementations of Is Even" /><published>2023-09-02T12:06:00+00:00</published><updated>2023-09-02T12:06:00+00:00</updated><id>http://0.0.0.0:4000/2023/09/02/is-even</id><content type="html" xml:base="http://0.0.0.0:4000/2023/09/02/is-even.html"><![CDATA[<p>I stumbled on this written as a joke somewhere on the web and found it hilariously terrible. <br />
What a way to turn a O(1) modulus operation into the funny abomination you see here.  <br />
This algorithm has a runtime of O(2^n). <br /></p>

<!--more-->

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">is_even</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
  <span class="n">x</span> <span class="o">=</span> <span class="nf">abs</span><span class="p">(</span><span class="n">num</span><span class="p">)</span>
  <span class="c1"># x is even if it can be divided into two parts y and z that equal each other
</span>  <span class="n">y</span><span class="p">,</span><span class="n">z</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span><span class="mi">0</span>
  <span class="c1"># take turns incrementing y and z
</span>  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="nf">is_even</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
      <span class="n">y</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">z</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="nf">return </span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">z</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Runtime(x) = Runtime(x-1) + Runtime(x-2) + ... + Runtime(0)  
Runtime(x-1) = Runtime(x-2) + Runtime(x-3) + ... + Runtime(0)  
Runtime(x) = 2 * Runtime(x-1) = 2 * 2 * Runtime(x-2) = ... = 2 * 2 * 2 * ... * Runtime(0) = 2^x * const. 

</code></pre></div></div>]]></content><author><name></name></author><category term="algorithms" /><summary type="html"><![CDATA[I stumbled on this written as a joke somewhere on the web and found it hilariously terrible. What a way to turn a O(1) modulus operation into the funny abomination you see here. This algorithm has a runtime of O(2^n).]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://0.0.0.0:4000/20231102_081138.jpg" /><media:content medium="image" url="http://0.0.0.0:4000/20231102_081138.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>